{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xor_with_neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JHJ4012/tensorflow_studying/blob/master/xor_with_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTRVAKrcXnLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc9b9add-fa4c-4fdf-c90b-4ebe3aeb745b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#잘 작동 안 됨\n",
        "# x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
        "# y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# X = tf.placeholder(tf.float32)\n",
        "# Y = tf.placeholder(tf.float32)\n",
        "# W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
        "# b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
        "\n",
        "# hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
        "\n",
        "# cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
        "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #0.5보다 크면 True, 작으면 False\n",
        "# accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#   for step in range(10001):\n",
        "#     sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
        "#     if step % 100 == 0:\n",
        "#       print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
        "\n",
        "#   h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "#   print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)\n",
        "\n",
        "#neural net\n",
        "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "#neural net 사용 부분\n",
        "W1 = tf.Variable(tf.random_normal([2,2]), name = 'weight1')\n",
        "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
        "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([2,1]), name = 'weight2')\n",
        "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
        "hypothesis = tf.sigmoid(tf.matmul(layer1,W) + b)\n",
        "\n",
        "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #0.5보다 크면 True, 작으면 False\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for step in range(10001):\n",
        "    sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
        "    if step % 100 == 0:\n",
        "      print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
        "\n",
        "  h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
        "  print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.83749616 [[0.87686014]\n",
            " [1.1288029 ]]\n",
            "100 0.6989605 [[0.4759111]\n",
            " [0.9451174]]\n",
            "200 0.6973584 [[0.44747946]\n",
            " [0.8861392 ]]\n",
            "300 0.69625145 [[0.43504643]\n",
            " [0.8425718 ]]\n",
            "400 0.69534385 [[0.42450258]\n",
            " [0.80905074]]\n",
            "500 0.6945716 [[0.4149769 ]\n",
            " [0.78451115]]\n",
            "600 0.69388664 [[0.40617603]\n",
            " [0.76829886]]\n",
            "700 0.6932497 [[0.3978758]\n",
            " [0.7599741]]\n",
            "800 0.69262695 [[0.38988966]\n",
            " [0.7592696 ]]\n",
            "900 0.6919865 [[0.38205802]\n",
            " [0.7660699 ]]\n",
            "1000 0.691296 [[0.37424302]\n",
            " [0.780394  ]]\n",
            "1100 0.69052017 [[0.3663267]\n",
            " [0.8023851]]\n",
            "1200 0.6896187 [[0.35821  ]\n",
            " [0.8323036]]\n",
            "1300 0.6885432 [[0.34981728]\n",
            " [0.8705213 ]]\n",
            "1400 0.6872341 [[0.34110332]\n",
            " [0.91751665]]\n",
            "1500 0.6856175 [[0.33206204]\n",
            " [0.97387403]]\n",
            "1600 0.6836006 [[0.32273942]\n",
            " [1.04028   ]]\n",
            "1700 0.6810678 [[0.31324118]\n",
            " [1.11752   ]]\n",
            "1800 0.677879 [[0.30374086]\n",
            " [1.2064531 ]]\n",
            "1900 0.67387235 [[0.29448056]\n",
            " [1.3079524 ]]\n",
            "2000 0.6688782 [[0.285774 ]\n",
            " [1.4227777]]\n",
            "2100 0.6627456 [[0.2780182]\n",
            " [1.5513632]]\n",
            "2200 0.6553824 [[0.2717255]\n",
            " [1.6935523]]\n",
            "2300 0.6467977 [[0.2675733]\n",
            " [1.8483443]]\n",
            "2400 0.63713 [[0.26645136]\n",
            " [2.0137813 ]]\n",
            "2500 0.6266421 [[0.26947945]\n",
            " [2.1870477 ]]\n",
            "2600 0.6156783 [[0.27797842]\n",
            " [2.3648005 ]]\n",
            "2700 0.60459596 [[0.29341203]\n",
            " [2.5436158 ]]\n",
            "2800 0.59369594 [[0.31733406]\n",
            " [2.7204285 ]]\n",
            "2900 0.5831752 [[0.35137346]\n",
            " [2.892838  ]]\n",
            "3000 0.5731049 [[0.39726856]\n",
            " [3.0592418 ]]\n",
            "3100 0.56342983 [[0.45693678]\n",
            " [3.2188356 ]]\n",
            "3200 0.55397433 [[0.53256404]\n",
            " [3.3715296 ]]\n",
            "3300 0.5444434 [[0.62670267]\n",
            " [3.5178504 ]]\n",
            "3400 0.53441316 [[0.74237376]\n",
            " [3.6588593 ]]\n",
            "3500 0.52330226 [[0.8831588]\n",
            " [3.7960978]]\n",
            "3600 0.5103319 [[1.0532357]\n",
            " [3.9315305]]\n",
            "3700 0.49448723 [[1.257236 ]\n",
            " [4.0674496]]\n",
            "3800 0.47453356 [[1.4997438]\n",
            " [4.2062297]]\n",
            "3900 0.44919035 [[1.7842246]\n",
            " [4.3499093]]\n",
            "4000 0.41761413 [[2.111168]\n",
            " [4.499634]]\n",
            "4100 0.38018942 [[2.475595 ]\n",
            " [4.6553235]]\n",
            "4200 0.33907467 [[2.8655183]\n",
            " [4.815783 ]]\n",
            "4300 0.29765537 [[3.2638671]\n",
            " [4.9791584]]\n",
            "4400 0.25909293 [[3.6536407]\n",
            " [5.1433754]]\n",
            "4500 0.22524352 [[4.0224624]\n",
            " [5.306436 ]]\n",
            "4600 0.19659737 [[4.363862 ]\n",
            " [5.4666314]]\n",
            "4700 0.17279187 [[4.676068 ]\n",
            " [5.6226377]]\n",
            "4800 0.15311985 [[4.960126]\n",
            " [5.77355 ]]\n",
            "4900 0.13683063 [[5.218421 ]\n",
            " [5.9188437]]\n",
            "5000 0.12325899 [[5.45376  ]\n",
            " [6.0582814]]\n",
            "5100 0.11185871 [[5.668922 ]\n",
            " [6.1918397]]\n",
            "5200 0.10219676 [[5.8664346]\n",
            " [6.3196335]]\n",
            "5300 0.09393473 [[6.0485287]\n",
            " [6.441867 ]]\n",
            "5400 0.08680911 [[6.217114 ]\n",
            " [6.5587935]]\n",
            "5500 0.08061378 [[6.373833 ]\n",
            " [6.6707034]]\n",
            "5600 0.07518713 [[6.52008  ]\n",
            " [6.7778716]]\n",
            "5700 0.07040079 [[6.6570487]\n",
            " [6.880586 ]]\n",
            "5800 0.06615243 [[6.7857533]\n",
            " [6.9791255]]\n",
            "5900 0.062359612 [[6.907063]\n",
            " [7.073752]]\n",
            "6000 0.05895534 [[7.021728]\n",
            " [7.164711]]\n",
            "6100 0.055884805 [[7.1303988]\n",
            " [7.2522364]]\n",
            "6200 0.05310271 [[7.233635 ]\n",
            " [7.3365417]]\n",
            "6300 0.050571546 [[7.33193  ]\n",
            " [7.4178257]]\n",
            "6400 0.048259694 [[7.425711 ]\n",
            " [7.4962726]]\n",
            "6500 0.046140704 [[7.515361 ]\n",
            " [7.5720534]]\n",
            "6600 0.044191882 [[7.6012106]\n",
            " [7.6453285]]\n",
            "6700 0.04239425 [[7.6835575]\n",
            " [7.7162385]]\n",
            "6800 0.040731158 [[7.7626677]\n",
            " [7.7849207]]\n",
            "6900 0.03918842 [[7.838778]\n",
            " [7.8515  ]]\n",
            "7000 0.0377538 [[7.9121 ]\n",
            " [7.91609]]\n",
            "7100 0.036416486 [[7.982826 ]\n",
            " [7.9788003]]\n",
            "7200 0.035167202 [[8.051125]\n",
            " [8.039727]]\n",
            "7300 0.03399763 [[8.117159]\n",
            " [8.098967]]\n",
            "7400 0.032900553 [[8.1810665]\n",
            " [8.156603 ]]\n",
            "7500 0.03186965 [[8.242977]\n",
            " [8.212712]]\n",
            "7600 0.030899141 [[8.303012]\n",
            " [8.26737 ]]\n",
            "7700 0.029984016 [[8.361277]\n",
            " [8.320648]]\n",
            "7800 0.029119778 [[8.41787 ]\n",
            " [8.372606]]\n",
            "7900 0.028302332 [[8.472883]\n",
            " [8.423309]]\n",
            "8000 0.027528023 [[8.5264015]\n",
            " [8.472811 ]]\n",
            "8100 0.026793668 [[8.578503]\n",
            " [8.521166]]\n",
            "8200 0.026096297 [[8.629259]\n",
            " [8.568423]]\n",
            "8300 0.0254331 [[8.678737]\n",
            " [8.61463 ]]\n",
            "8400 0.024801891 [[8.7269945]\n",
            " [8.659829 ]]\n",
            "8500 0.02420026 [[8.774096]\n",
            " [8.704062]]\n",
            "8600 0.023626305 [[8.820091]\n",
            " [8.74737 ]]\n",
            "8700 0.023078164 [[8.86503 ]\n",
            " [8.789786]]\n",
            "8800 0.022554113 [[8.90896 ]\n",
            " [8.831348]]\n",
            "8900 0.022052774 [[8.951924]\n",
            " [8.872085]]\n",
            "9000 0.021572527 [[8.993963]\n",
            " [8.912029]]\n",
            "9100 0.021112274 [[9.035115]\n",
            " [8.951211]]\n",
            "9200 0.020670637 [[9.075419]\n",
            " [8.989658]]\n",
            "9300 0.020246692 [[9.114908]\n",
            " [9.027394]]\n",
            "9400 0.019839302 [[9.153612]\n",
            " [9.064446]]\n",
            "9500 0.019447554 [[9.191562]\n",
            " [9.10084 ]]\n",
            "9600 0.019070592 [[9.228787]\n",
            " [9.136595]]\n",
            "9700 0.018707596 [[9.265314]\n",
            " [9.171731]]\n",
            "9800 0.018357866 [[9.301168]\n",
            " [9.20627 ]]\n",
            "9900 0.018020626 [[9.336373]\n",
            " [9.240232]]\n",
            "10000 0.017695261 [[9.370953]\n",
            " [9.273638]]\n",
            "\n",
            "Hypothesis:  [[0.02058211]\n",
            " [0.9848608 ]\n",
            " [0.9834187 ]\n",
            " [0.0178477 ]] \n",
            "Correct:  [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}